<?xml version="1.0" encoding="utf-8"?>
<!--
  For more information on how to configure your ASP.NET application, please visit
  https://go.microsoft.com/fwlink/?LinkId=169433
  -->
<configuration>
  <appSettings>
	<!-- Ajusta si tu LM Studio corre en otro host/puerto -->
	<add key="LLM_Studio_BaseUrl" value="http://192.168.1.27:1234" />
	<!-- Muchos builds de LM Studio exponen API OpenAI-compatible y no requieren API key -->
	<add key="LLM_Studio_ApiKey" value="" />
	<!-- Nombre del modelo cargado en LLM Studio (ej: "qwen2.5-7b-instruct", "llama-3.1-8b-instruct", etc.) -->
	<add key="LLM_Studio_Model" value="ibm/granite-4-h-tiny" />
    <!-- Ajustes de inferencia -->
    <add key="LLM_Temperature" value="0.2" />
    <add key="LLM_MaxTokens" value="1024" />
    <!-- Tu prompt base que viajará como 'system' -->
    <add key="LLM_SystemPrompt" value="Eres un asistente técnico. Responde claro y sin rodeos, en español neutro." />
  </appSettings>
  <system.web>
    <compilation debug="true" targetFramework="4.8.1" />
    <httpRuntime targetFramework="4.8.1" />
  </system.web>
  <system.codedom>
    <compilers>
      <compiler language="c#;cs;csharp" extension=".cs" warningLevel="4" compilerOptions="/langversion:default /nowarn:1659;1699;1701;612;618" type="Microsoft.CodeDom.Providers.DotNetCompilerPlatform.CSharpCodeProvider, Microsoft.CodeDom.Providers.DotNetCompilerPlatform, Version=4.1.0.0, Culture=neutral, PublicKeyToken=31bf3856ad364e35" />
      <compiler language="vb;vbs;visualbasic;vbscript" extension=".vb" warningLevel="4" compilerOptions="/langversion:default /nowarn:41008,40000,40008 /define:_MYTYPE=\&quot;Web\&quot; /optionInfer+" type="Microsoft.CodeDom.Providers.DotNetCompilerPlatform.VBCodeProvider, Microsoft.CodeDom.Providers.DotNetCompilerPlatform, Version=4.1.0.0, Culture=neutral, PublicKeyToken=31bf3856ad364e35" />
    </compilers>
  </system.codedom>
</configuration>